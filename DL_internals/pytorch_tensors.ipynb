{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kclx5ncpsJY4"
      },
      "source": [
        "# Pytorch Tensors\n",
        "Welcome! In this notebook you will learn what a **tensor** is, how to create and inspect tensors, and finally how to build a very small neural network (**MLP**) using torch. Each topic has a short text you can read first, then a code cell you can run.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hyperskill-content/hyperskill-ml-notebooks/blob/main/DL_internals/pytorch_tensors.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9XgotuTsJY8"
      },
      "source": [
        "## üöÄ Prerequisites\n",
        "\n",
        "Make sure you‚Äôre comfortable with the topics below before starting this notebook:\n",
        "\n",
        "| # | Topic (clickable links)|\n",
        "|---|-------|\n",
        "| 1 | **[Introduction to PyTorch](https://hyperskill.org/learn/step/34436)**  |\n",
        "| 2 | **[Basics of neural network architecture](https://hyperskill.org/learn/step/26997)** |\n",
        "| 3 | **[Activation functions](https://hyperskill.org/learn/step/35741)** |\n",
        "| 4 | **[Multilayer perceptron](https://hyperskill.org/learn/step/47283)** |\n",
        "\n",
        "*If you‚Äôre new to any item above, review it quickly, then dive back in here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_T24OdpsJY9"
      },
      "source": [
        "# üìë Table of Contents\n",
        "\n",
        "1. [What Is a Tensor?](#sec-tensor)  \n",
        "2. [Inspecting a Tensor](#sec-inspect)  \n",
        "3. [Creating Tensors](#sec-create)  \n",
        "4. [Indexing & Slicing](#sec-index)  \n",
        "5. [Reshaping & Transposing](#sec-reshape)  \n",
        "6. [In-Place vs Out-of-Place Ops](#sec-inplace)  \n",
        "7. [Broadcasting](#sec-broadcast)  \n",
        "8. [Mini Exercise ‚Äî RGB to Grayscale](#sec-grayscale)  \n",
        "9. [Tiny MLP](#sec-mlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w-AQHHcasJY-",
        "outputId": "9ef137fa-c371-475f-f047-5e3c2dc9d2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS9hGirCsJZA"
      },
      "source": [
        "### 1. What Is a Tensor?\n",
        "<a id=\"sec-tensor\"></a>\n",
        "A **tensor** is simply a multidimensional array of numbers.  \n",
        "* A single number is a **0-D tensor** (scalar).  \n",
        "* A list like `[1, 2, 3]` is a **1-D tensor** (vector).  \n",
        "* A table of numbers is a **2-D tensor** (matrix).  \n",
        "PyTorch stores tensors efficiently and lets us move them to GPU for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SpIoKllDsJZB",
        "outputId": "fa5d41ce-8a63-485b-817e-a3175c1f3092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scalar: tensor(7)\n",
            "vector: tensor([1, 2, 3])\n",
            "matrix:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ],
      "source": [
        "scalar = torch.tensor(7)                # 0-D\n",
        "vector = torch.tensor([1, 2, 3])        # 1-D\n",
        "matrix = torch.tensor([[1, 2], [3, 4]]) # 2-D\n",
        "print(\"scalar:\", scalar)\n",
        "print(\"vector:\", vector)\n",
        "print(\"matrix:\\n\", matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EDcYPAdsJZB"
      },
      "source": [
        "### 2. Inspecting a Tensor ‚Äî First Things to Check\n",
        "<a id=\"sec-inspect\"></a>\n",
        "Before any operation, ask three quick questions:\n",
        "\n",
        "| Property | Why you need it | How to get it |\n",
        "|----------|-----------------|---------------|\n",
        "| **shape** | does it fit the layer? | `x.shape` |\n",
        "| **dtype** | float, int, bool? | `x.dtype` |\n",
        "| **device** | CPU or GPU? | `x.device` |\n",
        "\n",
        "Bonus helpers:  \n",
        "* `x.ndim` ‚Üí number of dimensions  \n",
        "* `x.numel()` ‚Üí total elements  \n",
        "* `x.stride()` ‚Üí memory step size per dim (helps with speed issues)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GWkc32wusJZC",
        "outputId": "6a35f8fe-87f7-4e86-fe2e-96e32341ac94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape : torch.Size([2, 3, 4])\n",
            "dtype : torch.float32\n",
            "device: cpu\n",
            "ndim  : 3\n",
            "numel : 24\n",
            "stride: (12, 4, 1)\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 3, 4)\n",
        "print(\"shape :\", x.shape)\n",
        "print(\"dtype :\", x.dtype)\n",
        "print(\"device:\", x.device)\n",
        "print(\"ndim  :\", x.ndim)\n",
        "print(\"numel :\", x.numel())\n",
        "print(\"stride:\", x.stride())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3HkuZr7sJZD"
      },
      "source": [
        "### 3. Creating Tensors ‚Äî Your Toolkit\n",
        "<a id=\"sec-create\"></a>\n",
        "\n",
        "Use these factory functions every day:\n",
        "\n",
        "| Function | Typical use |\n",
        "|----------|-------------|\n",
        "| `torch.zeros` | initialise weights / masks |\n",
        "| `torch.ones`  | add bias of ones |\n",
        "| `torch.empty` | reserve memory fast (values uninitialised) |\n",
        "| `torch.full`  | constant tensors (e.g. padding value) |\n",
        "| `torch.arange`, `torch.linspace` | index vectors, positional encodings |\n",
        "\n",
        "*Real-world tip:* Random initialisation matters ‚Äî try different seeds when models misbehave.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i3EN4ge0sJZD",
        "outputId": "f22fe1ff-251d-4f70-ff95-1a0a1753ae06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zeros:\n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "range:\n",
            " tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "ones:\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "linspace:\n",
            " tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
            "full:\n",
            " tensor([[42, 42, 42],\n",
            "        [42, 42, 42]])\n"
          ]
        }
      ],
      "source": [
        "z = torch.zeros(2, 3)\n",
        "o = torch.ones(2, 3)\n",
        "r = torch.arange(0, 6).reshape(2, 3)\n",
        "l = torch.linspace(0, 1, steps=5)\n",
        "c = torch.full((2, 3), 42)\n",
        "print(\"zeros:\\n\", z)\n",
        "print(\"range:\\n\", r)\n",
        "print(\"ones:\\n\", o)\n",
        "print(\"linspace:\\n\", l)\n",
        "print(\"full:\\n\", c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTrv64S2sJZE"
      },
      "source": [
        "### 4. Indexing & Slicing ‚Äî Grabbing What You Need\n",
        "<a id=\"sec-index\"></a>\n",
        "\n",
        "Like NumPy, but with gradients.\n",
        "\n",
        "* Row / column selection  \n",
        "* Boolean masks for filtering  \n",
        "* Concatenation (`torch.cat`) and stacking (`torch.stack`) to merge data  \n",
        "  *Use case:* building mini-batches from separate tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i1Y4NgTksJZE",
        "outputId": "264a1d6f-f156-4451-8f2e-6f2f5b39e7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:\n",
            " tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "row 1 : tensor([4, 5, 6, 7])\n",
            "col 0 : tensor([0, 4, 8])\n",
            "evens : tensor([ 0,  2,  4,  6,  8, 10])\n",
            "cat shape: torch.Size([6, 4])\n",
            "stack shape: torch.Size([3, 2, 4])\n"
          ]
        }
      ],
      "source": [
        "a = torch.arange(12).reshape(3, 4)\n",
        "print(\"a:\\n\", a)\n",
        "print(\"row 1 :\", a[1])          # second row\n",
        "print(\"col 0 :\", a[:, 0])       # first column\n",
        "mask = a % 2 == 0\n",
        "print(\"evens :\", a[mask])       # boolean filter\n",
        "\n",
        "b = torch.ones_like(a)\n",
        "cat = torch.cat([a, b], dim=0)  # join rows\n",
        "stk = torch.stack([a, b], dim=1)# add new dim\n",
        "print(\"cat shape:\", cat.shape)\n",
        "print(\"stack shape:\", stk.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpA5RpdBsJZF"
      },
      "source": [
        "### 5. Reshaping & Transposing ‚Äî Same Data, New View\n",
        "<a id=\"sec-reshape\"></a>\n",
        "\n",
        "* `view`, `reshape` ‚Üí change shape without moving data (fast)  \n",
        "* `transpose`, `permute` ‚Üí reorder dimensions  \n",
        "* `.contiguous()` ‚Üí make memory continuous if needed\n",
        "\n",
        "**Use case:** Flatten images before a linear layer or swap channels for OpenCV (torch uses RGB and opencv BGR channel orders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "llFRHSl3sJZF",
        "outputId": "501a9094-5e74-4c5e-9010-81f029a93a6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix:\n",
            " tensor([[0, 1, 2, 3],\n",
            "        [4, 5, 6, 7]])\n",
            "transposed:\n",
            " tensor([[0, 4],\n",
            "        [1, 5],\n",
            "        [2, 6],\n",
            "        [3, 7]])\n",
            "is contiguous: False\n",
            "flattened: tensor([0, 4, 1, 5, 2, 6, 3, 7])\n"
          ]
        }
      ],
      "source": [
        "v = torch.arange(8)\n",
        "m = v.view(2, 4)        # 2√ó4\n",
        "t = m.T                 # 4√ó2\n",
        "print(\"matrix:\\n\", m)\n",
        "print(\"transposed:\\n\", t)\n",
        "print(\"is contiguous:\", t.is_contiguous())\n",
        "t_c = t.contiguous().view(-1)\n",
        "print(\"flattened:\", t_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSs392KLsJZF"
      },
      "source": [
        "### 6  In-Place (`*_`) vs. Out-of-Place Operations\n",
        "<a id=\"sec-inplace\"></a>\n",
        "\n",
        "* **Out-of-place** ‚Äì creates a **new tensor**, original stays the same.  \n",
        "* **In-place** ‚Äì ends with an underscore `_` and **changes data inside the same tensor**.  \n",
        "\n",
        "In-place saves memory but can break Autograd if you try it on a **leaf tensor** (a tensor you created with `requires_grad=True`).  \n",
        "PyTorch will stop you with the error:\n",
        "```\n",
        "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation\n",
        "```\n",
        "Below are quick demos:\n",
        "\n",
        "1. Safe out-of-place  \n",
        "2. In-place error on a leaf tensor  \n",
        "3. Two safe work-arounds  \n",
        "4. In-place on a non-leaf tensor (works)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cBufMc2_sJZF",
        "outputId": "bcb22810-eaa1-47b4-c3d4-160ab1b39c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([1., 2., 3.], requires_grad=True)\n",
            "y: tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 1Ô∏è‚É£  Safe out-of-place\n",
        "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "y = x + 1          # new tensor\n",
        "print(\"x:\", x)     # [1., 2., 3.]\n",
        "print(\"y:\", y)     # [2., 3., 4.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T_4mQ1yEsJZG",
        "outputId": "a9d39f1c-04df-4ad3-d732-8cb1cf45169a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4217199493.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2Ô∏è‚É£  In-place on a leaf tensor ‚Üí error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# raises RuntimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ],
      "source": [
        "# 2Ô∏è‚É£  In-place on a leaf tensor ‚Üí error\n",
        "z = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "z.add_(1)          # raises RuntimeError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rXVny3mosJZG",
        "outputId": "ba60cb6e-5e53-42a7-d0f7-7e8db30a03c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w: tensor([2., 3., 4.], requires_grad=True)\n",
            "u_det: tensor([2., 3., 4.])\n"
          ]
        }
      ],
      "source": [
        "# 3Ô∏è‚É£  Safe in-place methods\n",
        "\n",
        "# 3a. Use torch.no_grad()\n",
        "w = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    w.add_(1)\n",
        "print(\"w:\", w)     # [2., 3., 4.]\n",
        "\n",
        "# 3b. Detach first\n",
        "u = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "u_det = u.detach()\n",
        "u_det.add_(1)\n",
        "print(\"u_det:\", u_det)   # [2., 3., 4.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pJ0ETzXHsJZG",
        "outputId": "7ed154b7-cb6d-4c63-e920-061fcc47cb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root.grad: tensor([2., 2., 2.])\n"
          ]
        }
      ],
      "source": [
        "# 4Ô∏è‚É£  In-place on a non-leaf tensor (allowed)\n",
        "root  = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "child = root * 2            # not a leaf\n",
        "child.add_(1)               # in-place ok\n",
        "out = child.sum()\n",
        "out.backward()\n",
        "print(\"root.grad:\", root.grad)   # tensor([2., 2., 2.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJtuSwUsJZH"
      },
      "source": [
        "**Key points**\n",
        "\n",
        "* You can always use out-of-place ops‚Äîthey‚Äôre simple and safe, unless you have a memory limitations.\n",
        "* If you really need in-place, wrap it in `torch.no_grad()` or use `.detach()`.  \n",
        "* PyTorch optimisers already handle in-place parameter updates for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4nWI4GAsJZH"
      },
      "source": [
        "### 7. Broadcasting ‚Äî Math with Different Shapes\n",
        "<a id=\"sec-broadcast\"></a>\n",
        "Broadcasting lets PyTorch stretch a smaller tensor to match a larger one.\n",
        "\n",
        "**Use case:** Add channel-wise mean / std when normalising images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xDZiDNEBsJZH",
        "outputId": "a5542746-771d-4c43-ca39-07ee8b15f484",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2695, 0.5484, 0.7490],\n",
            "        [1.3357, 0.7296, 0.0572]])\n"
          ]
        }
      ],
      "source": [
        "mat = torch.rand(2, 3)\n",
        "vec = torch.tensor([1.0, 0.5, 0.0])   # shape (3,)\n",
        "res = mat + vec                       # vec ‚Üí (2,3)\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYyU39IrsJZH"
      },
      "source": [
        "### 8. Mini Exercise ‚Äî From RGB to Grayscale\n",
        "<a id=\"sec-grayscale\"></a>\n",
        "\n",
        "Goal:  \n",
        "*Take a batch of RGB images, convert each one to a single-channel grayscale image, then check the new shape.*\n",
        "\n",
        "Why it matters:  \n",
        "Many computer-vision tasks (e.g., edge detection, classic ML models) need **1-channel** input instead of color.  \n",
        "Knowing how to reduce channels is a handy tensor skill.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jJcmOQ8ysJZH",
        "outputId": "8294bdaf-97e7-4430-cd70-1fa044c2b6ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original shape: torch.Size([16, 3, 128, 128])\n",
            "grayscale shape: torch.Size([16, 1, 128, 128])\n",
            "min/max: 0.006300687789916992 0.9897307753562927\n"
          ]
        }
      ],
      "source": [
        "# Batch of 16 RGB images with pixel values in [0, 1]\n",
        "imgs = torch.rand(16, 3, 128, 128)        # shape (B, C, H, W)\n",
        "\n",
        "# Average the 3 colour channels ‚Üí grayscale\n",
        "gray = imgs.mean(dim=1, keepdim=True)     # shape becomes (B, 1, H, W)\n",
        "\n",
        "print(\"original shape:\", imgs.shape)\n",
        "print(\"grayscale shape:\", gray.shape)\n",
        "\n",
        "# Optional: verify range still [0, 1]\n",
        "print(\"min/max:\", gray.min().item(), gray.max().item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWPqAn_sJZH"
      },
      "source": [
        "## 9Ô∏è‚É£ Tiny MLP ‚Äî How It Works\n",
        "\n",
        "<a id=\"sec-mlp\"></a>\n",
        "\n",
        "A **Multilayer Perceptron (MLP)** is the hello-world of neural networks.  \n",
        "Ours has three main parts:\n",
        "\n",
        "| Part | What it does | Why it matters |\n",
        "|------|--------------|----------------|\n",
        "| **Flatten layer** | Turns a 3-D image `(C √ó H √ó W)` into a 1-D vector | Linear layers need 1-D input |\n",
        "| **Linear + ReLU (hidden layer)** | `x ‚Üí Wx + b`, then keep only positive values | Learns mixed features (edges, colours, ‚Ä¶) |\n",
        "| **Linear (output layer)** | New weights turn hidden features into **logits** (raw scores) | One logit per class (10 here) |\n",
        "| **Softmax (optional)** | Converts logits to probabilities that sum to 1 | Easier to read & plot |\n",
        "\n",
        "> **Why just two layers?**  \n",
        "> Small models train fast and make debugging easy. Once the flow is clear, you can add more layers or switch to ConvNets.\n",
        "\n",
        "In the demo below you will:\n",
        "\n",
        "1. Watch shapes change through the network  \n",
        "2. Turn logits into probabilities  \n",
        "3. Visualise the predicted class distribution for one sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7hWHwgnXsJZI",
        "outputId": "bcfcded7-8c8d-4b76-ac78-96331e7435ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input   : torch.Size([4, 3, 224, 224])\n",
            "flatten : torch.Size([4, 150528])\n",
            "hidden  : torch.Size([4, 128])\n",
            "logits  : torch.Size([4, 10])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ----- Tiny MLP definition -----\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, in_features=3*224*224, hidden=128, out=10):\n",
        "        super().__init__()\n",
        "        self.flat = nn.Flatten()              # 1Ô∏è‚É£ flatten\n",
        "        self.hidden = nn.Linear(in_features, hidden)\n",
        "        self.act = nn.ReLU()\n",
        "        self.out = nn.Linear(hidden, out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"input   :\", x.shape)            # (B, 3, 224, 224)\n",
        "        x = self.flat(x)\n",
        "        print(\"flatten :\", x.shape)            # (B, 150 528)\n",
        "        x = self.act(self.hidden(x))\n",
        "        print(\"hidden  :\", x.shape)            # (B, 128)\n",
        "        logits = self.out(x)\n",
        "        print(\"logits  :\", logits.shape)       # (B, 10)\n",
        "        return logits\n",
        "\n",
        "# ----- Fake batch -----\n",
        "batch = torch.rand(4, 3, 224, 224)            # 4 RGB images\n",
        "\n",
        "model = TinyMLP()\n",
        "logits = model(batch)                         # forward pass\n",
        "probs  = logits.softmax(dim=1)                # nice probabilities\n",
        "\n",
        "# ----- Visualise probs for the first sample -----\n",
        "classes = [f\"C{i}\" for i in range(10)]\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.bar(classes, probs[0].detach().numpy())\n",
        "plt.title(\"Class probabilities (sample 0)\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfppP2UDsJZI"
      },
      "source": [
        "### 9.1  Autograd in Action ‚Äî One Training Step\n",
        "\n",
        "`Autograd` is PyTorch‚Äôs automatic-differentiation engine.  \n",
        "When you call `.backward()`, PyTorch walks back through the graph and fills `.grad` for each parameter.\n",
        "\n",
        "Below we will:\n",
        "\n",
        "1. Create **fake labels**.  \n",
        "2. Compute **cross-entropy loss** on the logits from the Tiny MLP.  \n",
        "3. Call `loss.backward()` and inspect one gradient tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rmdf34jJsJZI",
        "outputId": "feddea57-f1c4-43a2-8d2f-4bc10b7d8719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.370863199234009\n",
            "hidden.weight.grad shape: torch.Size([128, 150528])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# ----- fake labels (0-9) -----\n",
        "targets = torch.randint(0, 10, (logits.size(0),))       # shape (B,)\n",
        "\n",
        "# ----- loss & backward -----\n",
        "loss = F.cross_entropy(logits, targets)\n",
        "print(\"loss:\", loss.item())\n",
        "\n",
        "loss.backward()    # gradients now stored in model parameters\n",
        "\n",
        "# show gradient of first weight matrix\n",
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(f\"{name}.grad shape:\", p.grad.shape)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufmgpzjnsJZJ"
      },
      "source": [
        "### 9.2  Moving Data & Model to GPU\n",
        "\n",
        "If you have an NVIDIA GPU, you can run the same network **much faster**.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Pick a device: `\"cuda\"` if available, else `\"cpu\"`.  \n",
        "2. `.to(device)` on **both** data **and** model.  \n",
        "3. Run the forward pass.  (Optionally, time it.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Oy6GAytusJZJ",
        "outputId": "eeb400bc-ede6-48c2-8c10-24deaaef7788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "input   : torch.Size([8, 3, 224, 224])\n",
            "flatten : torch.Size([8, 150528])\n",
            "hidden  : torch.Size([8, 128])\n",
            "logits  : torch.Size([8, 10])\n",
            "output shape: torch.Size([8, 10])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# move model and a new dummy batch\n",
        "model_gpu = TinyMLP().to(device)\n",
        "batch_gpu  = torch.rand(8, 3, 224, 224, device=device)\n",
        "\n",
        "# quick timing\n",
        "with torch.no_grad():\n",
        "    logits_gpu = model_gpu(batch_gpu)        # forward on GPU / CPU\n",
        "print(\"output shape:\", logits_gpu.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgpHWe4MsJZJ"
      },
      "source": [
        "### 9.3  Putting It All Together\n",
        "\n",
        "* You now know how to **create a model**, **move it to GPU**,  \n",
        "  **run a forward pass**, **compute loss**, and **back-propagate gradients**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToLCWDkNsJZJ"
      },
      "source": [
        "#### Exercise 1 ‚Äì Build a 2 √ó 3 Tensor of Zeros  \n",
        "\n",
        "Create a tensor named **`t`** with shape `(2, 3)`, all zeros, `dtype=torch.float32`, living on the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8xEEKh8GsJZJ",
        "outputId": "ecdfbbbd-1975-4765-a929-53626e2fc7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exercise 1 passed!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# üöß Your code below\n",
        "# t = ...\n",
        "t = torch.zeros(2, 3, dtype=torch.float32)\n",
        "# üîé Tests\n",
        "assert isinstance(t, torch.Tensor), \"Not a tensor\"\n",
        "assert t.shape == (2, 3), \"Shape should be (2, 3)\"\n",
        "assert t.dtype == torch.float32, \"dtype must be float32\"\n",
        "assert t.device.type == \"cpu\", \"Keep it on CPU for this test\"\n",
        "assert torch.all(t == 0), \"Tensor must contain only zeros\"\n",
        "print(\"‚úÖ Exercise 1 passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrn1cYREsJZJ"
      },
      "source": [
        "#### Exercise 2 ‚Äì Reshape a Vector to 3 √ó 4  \n",
        "\n",
        "Given the 1-D tensor `v` below, create `m` with shape `(3, 4)` **using `view`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w8ItTme1sJZK",
        "outputId": "12ab8ac1-974a-44e6-a679-ff40fb6ba3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exercise 2 passed!\n"
          ]
        }
      ],
      "source": [
        "v = torch.arange(12)           # [0 ‚Ä¶ 11]\n",
        "# üöß Your code below\n",
        "m = v.view(3,4)\n",
        "\n",
        "# üîé Tests\n",
        "assert m.shape == (3, 4), \"Shape should be (3, 4)\"\n",
        "assert m.is_contiguous(), \"Use view, not transpose\"\n",
        "assert torch.equal(m.flatten(), v), \"Values must match original order\"\n",
        "print(\"‚úÖ Exercise 2 passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Yju87hsJZK"
      },
      "source": [
        "#### Exercise 3 ‚Äì Stack Along a New Dimension  \n",
        "\n",
        "Stack `a` and `b` **along a new leading dimension** so the result has shape `(2, 3, 4)`.  \n",
        "(Do **not** use a Python list; use `torch.stack`.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aZdsEcpasJZK",
        "outputId": "92bffa12-4d17-4712-dadc-e804bbb0acc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exercise 3 passed!\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(3, 4)\n",
        "b = torch.zeros(3, 4)\n",
        "# üöß Your code below\n",
        "out = torch.stack([a, b], dim=0)\n",
        "\n",
        "# üîé Tests\n",
        "assert out.shape == (2, 3, 4), \"Result should be (2, 3, 4)\"\n",
        "assert (out[0] == 1).all() and (out[1] == 0).all(), \"Order should be [a, b]\"\n",
        "print(\"‚úÖ Exercise 3 passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh2DinfXsJZK"
      },
      "source": [
        "#### Exercise 4 ‚Äì Convert to Grayscale  \n",
        "\n",
        "Given a batch `imgs` of shape `(N, 3, H, W)` with values in `[0, 1]`,  \n",
        "create **`gray`** with shape `(N, 1, H, W)` by averaging the colour channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sa5DhP9QsJZK",
        "outputId": "09f9c582-f1d5-409e-9cbb-9272e543832c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exercise 4 passed!\n"
          ]
        }
      ],
      "source": [
        "imgs = torch.rand(5, 3, 64, 64)   # fake batch\n",
        "# üöß Your code below\n",
        "gray = imgs.mean(dim=1, keepdim=True)\n",
        "\n",
        "# üîé Tests\n",
        "assert gray.shape == (5, 1, 64, 64), \"Wrong shape\"\n",
        "assert (gray >= 0).all() and (gray <= 1).all(), \"Keep values in [0, 1]\"\n",
        "# spot-check mean\n",
        "assert torch.allclose(gray[0,0,0,0], imgs[0,:,0,0].mean()), \"Averaging mismatch\"\n",
        "print(\"‚úÖ Exercise 4 passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jIx6VBJsJZQ"
      },
      "source": [
        "#### Exercise 5 ‚Äì Device Transfer  \n",
        "\n",
        "Move tensor `x` and the TinyMLP `model` to **`cuda`** *if* a GPU is available,  \n",
        "otherwise leave them on CPU.  Store the result in `x_dev` and `model_dev`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BG5pEjHDsJZQ",
        "outputId": "ea71c5ac-5477-45d0-e7bb-df31fc63f067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exercise 5 passed!\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 3, 224, 224)\n",
        "model = TinyMLP()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# üöß Your code below\n",
        "x_dev = x.to(device)\n",
        "model_dev = model.to(device)\n",
        "\n",
        "# üîé Tests\n",
        "assert x_dev.device.type == device.type, \"x_dev on wrong device\"\n",
        "assert next(model_dev.parameters()).device.type == device.type, \"model_dev on wrong device\"\n",
        "print(\"‚úÖ Exercise 5 passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKEsmbV3sJZQ"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "To work confidently with data in PyTorch, start by **creating tensors** with helper functions such as `torch.zeros`, `torch.ones`, or `torch.rand`, then **inspect** them with `.shape`, `.dtype`, and `.device` to catch shape or type errors early.  \n",
        "When you need a new view of the same data, use `view` / `reshape`; if you must reorder dimensions, rely on `transpose` or `permute`, and call `.contiguous()` before reshaping if the stride is disrupted.\n",
        "\n",
        "Slice and index tensors just as you would in NumPy, or merge them with `torch.cat` and `torch.stack` to build mini-batches.  \n",
        "Remember that **out-of-place operations** create new tensors and are always safe, while **in-place operations** (ending in `_`) save memory but cannot be applied to leaf tensors that require gradients without wrapping them in `torch.no_grad()` or detaching first.\n",
        "\n",
        "Use **broadcasting** to let PyTorch align smaller tensors automatically‚Äîideal for channel-wise operations like converting an RGB batch to a single-channel grayscale batch by averaging along `dim=1`.\n",
        "\n",
        "After preprocessing, **flatten images** before a `Linear` layer, pass them through a hidden layer with `ReLU`, and produce logits with an output layer to form a **tiny MLP**.  \n",
        "Compute loss with `torch.nn.functional.cross_entropy`, call `loss.backward()` to fill parameter gradients, and move both data and model to `\"cuda\"` when a GPU is available to accelerate training.\n",
        "\n",
        "These steps‚Äîtensor creation, inspection, reshaping, safe memory operations, broadcasting, simple preprocessing, Autograd, and GPU transfer‚Äîform the everyday toolkit for building and debugging neural-network pipelines in PyTorch.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}